{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"Amazon-Deutsch-Dataset.csv\")\n",
    "df = df[[\"content\", \"rating\"]]\n",
    "df.rating= df.rating.str[0]\n",
    "df = df.dropna()\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "# create iterator from tokenized df\n",
    "def df_iterator_content(df):\n",
    "    for _, row in df.iterrows():\n",
    "        yield tokenizer(row['content'])\n",
    "\n",
    "vocab = build_vocab_from_iterator(df_iterator_content(df), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def text_pipeline(x):                           \n",
    "    return vocab(tokenizer(x))\n",
    "\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  28,  212,  472,    6,  122,  656,   18,   18,    0,    0],\n",
       "         [ 778,    0,    6,   61,  145, 2043,    2,   26,  146,  129]]),\n",
       " tensor([4, 2]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch, word_count=500):\n",
    "    label_list, text_list=[], []\n",
    "    for (text, label) in batch:\n",
    "        label_list.append(label_pipeline(label))\n",
    "        words=text_pipeline(text)\n",
    "        if len(words) > word_count:\n",
    "            words=words[:word_count]\n",
    "        else:\n",
    "            words.extend([0]*(word_count-len(words)))\n",
    "        text_list.append(words)\n",
    "    return torch.tensor(text_list), torch.tensor(label_list)\n",
    "\n",
    "# check if collate_batch works\n",
    "collate_batch([(\"Das neue IPhone ist wirklich toll!!\", \"5\"), (\"FritzBox 7830 ist schon ganz nett, aber geht besser\", \"3\")], word_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_one_hot(batch, num_classes):\n",
    "    return torch.zeros(batch.shape[0], batch.shape[1], num_classes).scatter_(2, batch.unsqueeze(2), 1)\n",
    "\n",
    "xb, yb=collate_batch([(\"Das neue IPhone ist wirklich toll!!\", \"5\"), (\"FritzBox 7830 ist schon ganz nett, aber geht besser\", \"3\")], word_count=10)\n",
    "idx_to_one_hot(xb, vocab_size)\n",
    "\n",
    "def collate_batch_with_one_hot(batch, word_count=500, vocab_size=10000):\n",
    "    xb, yb=collate_batch(batch, word_count)\n",
    "    return idx_to_one_hot(xb, vocab_size), yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        print(\"xb shape\", xb.shape)\n",
    "        embeds = self.word_embeddings(xb)\n",
    "        lstm_out, _ = self.lstm(embeds.view(xb.shape[1], 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(xb.shape[1], -1))\n",
    "        output = F.log_softmax(tag_space, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yb tensor([4, 2])\n",
      "xb torch.Size([2, 500, 22392])\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 32\n",
    "num_class = 2\n",
    "hidden_dim = 32\n",
    "\n",
    "model= MyLSTM(embed_dim, hidden_dim, vocab_size, num_class)\n",
    "\n",
    "# check if model works\n",
    "xb, yb=collate_batch_with_one_hot([(\"Das neue IPhone ist wirklich toll!!\", \"5\"), (\"FritzBox 7830 ist schon ganz nett, aber geht besser\", \"3\")], vocab_size=vocab_size)\n",
    "\n",
    "print(\"yb\", yb)\n",
    "print(\"xb\", xb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "train_dataset=df[['content', 'rating']].values;\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "# import torch DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "model = MyLSTM(embed_dim, hidden_dim, vocab_size, num_class).to(device)\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch_with_one_hot)\n",
    "\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,collate_fn=collate_batch_with_one_hot)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3.0, gamma=0.7)   # every 3 epochs, LR is multiplied by 0.7\n",
    "total_accu = None\n",
    "\n",
    "train_accus=[]\n",
    "valid_accus=[]\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    for idx, (text, label) in enumerate(train_dataloader):\n",
    "        print(text.shape)\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = loss_func(predicted_label, label)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "    accu_train = evaluate(model, train_dataloader)\n",
    "    accu_valid = evaluate(model, valid_dataloader)\n",
    "    train_accus.append(accu_train)\n",
    "    valid_accus.append(accu_valid)\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | train accuracy {:8.3f} | valid accuracy {:8.3f} | lr: {:1.2f}'.format(\n",
    "                                epoch,\n",
    "                                time.time() - epoch_start_time,\n",
    "                                accu_train, \n",
    "                                accu_valid, \n",
    "                                scheduler.get_last_lr()[0]))\n",
    "\n",
    "    scheduler.step() # learning rate scheduler after each epoch\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_accus, label='train_accu')\n",
    "plt.plot(valid_accus, label='valid_accu')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
