{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2399\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"Amazon-Deutsch-Dataset.csv\")\n",
    "df = df[[\"content\", \"rating\"]]\n",
    "df.rating= df.rating.str[0]\n",
    "df = df.dropna()\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "# create iterator from tokenized df\n",
    "def df_iterator_content(df):\n",
    "    for _, row in df.iterrows():\n",
    "        yield tokenizer(row['content'])\n",
    "\n",
    "vocab = build_vocab_from_iterator(df_iterator_content(df), specials=[\"<unk>\"], min_freq=10)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AmazonDataset(Dataset):\n",
    "    def __init__(self, df, word_count=500, vocab_size=10000):\n",
    "        self.df = df\n",
    "        self.word_count = word_count\n",
    "        self.vocab_size = vocab_size\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        x= self.df.iloc[idx][\"content\"]\n",
    "        y= self.df.iloc[idx][\"rating\"]\n",
    "        y = int(y) - 1\n",
    "        x = vocab(tokenizer(x))\n",
    "        if len(x) > self.word_count:\n",
    "            x=x[:self.word_count]\n",
    "        else:\n",
    "            x.extend([0]*(self.word_count-len(x)))\n",
    "        x = torch.tensor(x)\n",
    "        return x, y\n",
    "\n",
    "amazon_dataset = AmazonDataset(df, word_count=100, vocab_size=vocab_size)\n",
    "x,y=amazon_dataset[0]\n",
    "print(x.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, out_size, word_count=50):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=0.2)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to output space\n",
    "        self.hidden2output = nn.Linear(hidden_dim*word_count, out_size)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        #print(\"xb shape\", xb.shape)\n",
    "        embeds = self.word_embeddings(xb)\n",
    "        #print(\"embeds shape\", embeds.shape)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        #print(\"lstm_out shape\", lstm_out.shape)\n",
    "        # lstm_out_view = lstm_out[:, -1, :]   # works but looses information\n",
    "        lstm_out_view = lstm_out.reshape(xb.shape[0], -1   )\n",
    "        #print(\"lstm_out_view shape\", lstm_out_view.shape)\n",
    "        hidden_space = self.hidden2output(lstm_out_view)\n",
    "        #print(\"hidden_space shape\", hidden_space.shape)\n",
    "        output = F.log_softmax(hidden_space, dim=1)\n",
    "        #print(\"output shape\", output.shape)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yb tensor([2, 3, 3, 3, 0])\n",
      "xb torch.Size([5, 200])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bernd\\anaconda3_neu\\envs\\torch_etc\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[-1.6160, -1.6460, -1.4634, -1.5387, -1.8178],\n        [-1.6355, -1.6236, -1.5338, -1.7346, -1.5335],\n        [-1.6314, -1.6048, -1.5299, -1.5347, -1.7641],\n        [-1.6264, -1.6392, -1.5456, -1.5011, -1.7537],\n        [-1.6041, -1.5927, -1.5062, -1.5007, -1.8915]],\n       grad_fn=<LogSoftmaxBackward0>)"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 32\n",
    "num_class = 5\n",
    "hidden_dim = 32\n",
    "word_count = 200\n",
    "\n",
    "model= MyLSTM(embed_dim, hidden_dim, vocab_size, num_class, word_count=word_count)\n",
    "dataset = AmazonDataset(df, word_count=word_count, vocab_size=vocab_size)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# check if model works\n",
    "xb, yb = next(iter(loader))\n",
    "\n",
    "print(\"yb\", yb)\n",
    "print(\"xb\", xb.shape)\n",
    "model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (text, label) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and valid dataset\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [int(len(dataset)*0.8), len(dataset)-int(len(dataset)*0.8)])\n",
    "\n",
    "# import torch DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30 # epoch\n",
    "LR = 0.01  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "model = MyLSTM(embed_dim, hidden_dim, vocab_size, num_class, word_count=word_count)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10.0, gamma=0.7)   # every 10 epochs, LR is multiplied by 0.7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 18.75s | train accuracy    0.556 | valid accuracy    0.400 | lr: 0.01\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 16.44s | train accuracy    0.696 | valid accuracy    0.455 | lr: 0.01\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 16.80s | train accuracy    0.797 | valid accuracy    0.444 | lr: 0.01\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 16.96s | train accuracy    0.871 | valid accuracy    0.456 | lr: 0.01\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 20.06s | train accuracy    0.902 | valid accuracy    0.438 | lr: 0.01\n"
     ]
    }
   ],
   "source": [
    "total_accu = None\n",
    "train_accus=[]\n",
    "valid_accus=[]\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    for idx, (text, label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = loss_func(predicted_label, label)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "    accu_train = evaluate(model, train_dataloader)\n",
    "    accu_valid = evaluate(model, valid_dataloader)\n",
    "    train_accus.append(accu_train)\n",
    "    valid_accus.append(accu_valid)\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | train accuracy {:8.3f} | valid accuracy {:8.3f} | lr: {:1.2f}'.format(\n",
    "                                epoch,\n",
    "                                time.time() - epoch_start_time,\n",
    "                                accu_train, \n",
    "                                accu_valid, \n",
    "                                scheduler.get_last_lr()[0]))\n",
    "\n",
    "    scheduler.step() # learning rate scheduler after each epoch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_accus, label='train_accu')\n",
    "plt.plot(valid_accus, label='valid_accu')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# how much valid accuracy do we get in a new untrained model?\n",
    "new_model = MyLSTM(embed_dim, hidden_dim, vocab_size, num_class)\n",
    "evaluate(new_model, valid_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
